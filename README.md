# AI Hand Tracking & Gesture Recognition (Teachable Machine)
**Real-time Hand Gesture Recognition System using MediaPipe & KNN Classifier**

---

### üåê Project Information
**Subject:** Introduction to Artificial Intelligence (‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)  
**Program:** B.Ind.Tech in Electrical Technology and Smart Control Systems (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏≠‡∏™.‡∏ö.‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞)  
**Department:** Electrical Engineering, Faculty of Industry and Technology  
**University:** Rajamangala University of Technology Isan, Sakon Nakhon Campus (‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏£‡∏≤‡∏ä‡∏°‡∏á‡∏Ñ‡∏•‡∏≠‡∏µ‡∏™‡∏≤‡∏ô ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏Ç‡∏ï‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£)

**Developer / Instructor:** Nakarin Sripanya (‡∏≠.‡∏ô‡∏Ñ‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏®‡∏£‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤)  
**Email:** nakatin.sr@rmuti.ac.th  
**GitHub:** [https://github.com/NKSR22/AI-Hand-Gesture-Recognition](https://github.com/NKSR22/AI-Hand-Gesture-Recognition)

---

## üìñ About the Project (‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)
**[EN]** This project is a real-time hand gesture recognition system developed using Python, MediaPipe, and OpenCV. It features a **"Teachable Machine"** capability, allowing users to train the AI to recognize custom hand gestures instantly using a K-Nearest Neighbors (KNN) algorithm without retraining the entire model.

**[TH]** ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏ö‡∏ô‡∏¥‡πâ‡∏ß‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Python, MediaPipe ‡πÅ‡∏•‡∏∞ OpenCV ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡∏ö **"Teachable Machine"** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ "‡∏™‡∏≠‡∏ô" AI ‡πÉ‡∏´‡πâ‡∏à‡∏î‡∏à‡∏≥‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà‡πÜ ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° KNN ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô

---

## üß† Theory & Principles (‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)

### 1. Computer Vision & Landmark Detection
**[EN]** The system utilizes **MediaPipe Hands**, a high-fidelity hand tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from a single video frame.
- **Palm Detection Model:** Operates on the full image and returns an oriented hand bounding box.
- **Hand Landmark Model:** Operates on the cropped image region defined by the palm detector and returns high-fidelity 3D hand keypoints.

**[TH]** ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ **MediaPipe Hands** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Machine Learning ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏∏‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î 3 ‡∏°‡∏¥‡∏ï‡∏¥ (3D Landmarks) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 21 ‡∏à‡∏∏‡∏î‡∏ö‡∏ô‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
- **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ù‡πà‡∏≤‡∏°‡∏∑‡∏≠:** ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏°‡∏∑‡∏≠‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
- **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏°‡∏∑‡∏≠:** ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏∑‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

![Hand Landmarks](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png)

### 2. Feature Extraction (‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞)
**[EN]** Raw coordinates (x, y) from the camera are not suitable for direct classification due to position and scale variations. The system performs preprocessing:
- **Translation Invariance:** All points are shifted relative to the Wrist (Point 0) so that the hand's position on the screen doesn't affect recognition.
- **Scale Invariance:** Coordinates are normalized by the hand's size to ensure consistent recognition regardless of distance from the camera.

**[TH]** ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏î‡∏¥‡∏ö (x, y) ‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÅ‡∏Å‡πà‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:
- **‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (Translation Invariance):** ‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏∑‡∏≠ (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 0) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏°‡∏∑‡∏≠‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å
- **‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î (Scale Invariance):** ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ù‡πà‡∏≤‡∏°‡∏∑‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏°‡∏∑‡∏≠‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏Å‡∏•‡∏Å‡∏•‡πâ‡∏≠‡∏á

### 3. K-Nearest Neighbors (KNN) Classification
**[EN]** For the "Teachable" feature, the system uses the **K-Nearest Neighbors (KNN)** algorithm. It is a non-parametric, lazy learning algorithm.
- When a user saves a gesture, the extracted feature vector is stored in memory.
- During inference, the system calculates the **Euclidean Distance** between the current hand pose and all stored samples.
- It selects the `K` closest samples (Neighbors) and assigns the class label based on a majority vote.

**[TH]** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå "‡∏™‡∏≠‡∏ô‡πÑ‡∏î‡πâ" ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° **K-Nearest Neighbors (KNN)** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö Lazy Learning
- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Feature Vector) ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
- ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏¢‡∏π‡∏Ñ‡∏•‡∏¥‡∏î (Euclidean Distance)** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡πà‡∏≤‡∏°‡∏∑‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô `K` ‡∏ï‡∏±‡∏ß (Neighbors) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å (Majority Vote)

---

## üõ† Installation & Usage (‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)

### Prerequisites (‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ)
- Python 3.9+
- Webcam

### Steps (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)
1. **Clone Repository:**
   ```bash
   git clone https://github.com/NKSR22/AI-Hand-Gesture-Recognition.git
   cd AI_Count_figer
   ```

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Application:**
   ```bash
   python main.py
   ```

### üéÆ How to Use (‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
- **Heuristic Mode (Default):** The system counts fingers using standard geometry logic (Yellow status).
- **Training Mode:**
  1. Pose your hand in front of the camera.
  2. Press a number key **(0-9)** to label that pose.
  3. The system switches to **AI Mode** (Green status) and starts recognizing your custom gestures.
- **Clear Data:** Press **'C'** to reset training data.
- **Exit:** Click the **EXIT button** or press **'Q'**.

---

## üê≥ Docker Support (Advanced)
This project includes a `Dockerfile` for containerization. Note that running GUI applications with Webcam access via Docker requires specific X11 forwarding configurations.

1. **Build Image:** `docker build -t ai-hand-tracker .`
2. **Run Container:** Commands vary by OS (see Docker documentation or previous guides).

---
**¬© 2024 Nakarin Sripanya.** All Rights Reserved.
