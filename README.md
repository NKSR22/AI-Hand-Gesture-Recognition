# AI Hand Tracking & Gesture Recognition (Teachable Machine)
**Real-time Hand Gesture Recognition System using MediaPipe & KNN Classifier**

---

### üåê Project Information
**Subject:** Introduction to Artificial Intelligence (‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)  
**Program:** B.Ind.Tech in Electrical Technology and Intelligence Control Systems (‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏≠‡∏™.‡∏ö.‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞)  
**Department:** Electrical Engineering, Faculty of Industry and Technology  
**University:** Rajamangala University of Technology Isan, Sakon Nakhon Campus (‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏£‡∏≤‡∏ä‡∏°‡∏á‡∏Ñ‡∏•‡∏≠‡∏µ‡∏™‡∏≤‡∏ô ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏Ç‡∏ï‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£)

**Developer / Instructor:** Nakarin Sripanya (‡∏≠.‡∏ô‡∏Ñ‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏®‡∏£‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤)  
**Email:** nakatin.sr@rmuti.ac.th  
**GitHub:** [https://github.com/NKSR22/AI-Hand-Gesture-Recognition](https://github.com/NKSR22/AI-Hand-Gesture-Recognition)

---

## üìñ About the Project (‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)
**[EN]** This project is a real-time hand gesture recognition system developed using Python, MediaPipe, and OpenCV. It features a **"Teachable Machine"** capability, allowing users to train the AI to recognize custom hand gestures instantly using a K-Nearest Neighbors (KNN) algorithm without retraining the entire model.

**[TH]** ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏ö‡∏ô‡∏¥‡πâ‡∏ß‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Python, MediaPipe ‡πÅ‡∏•‡∏∞ OpenCV ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡∏ö **"Teachable Machine"** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ "‡∏™‡∏≠‡∏ô" AI ‡πÉ‡∏´‡πâ‡∏à‡∏î‡∏à‡∏≥‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà‡πÜ ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° KNN ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô

---

## üß† Theory & Principles (‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)

### 1. Computer Vision & Landmark Detection
**[EN]** The system utilizes **MediaPipe Hands**, a high-fidelity hand tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from a single video frame.
- **Palm Detection Model:** Operates on the full image and returns an oriented hand bounding box.
- **Hand Landmark Model:** Operates on the cropped image region defined by the palm detector and returns high-fidelity 3D hand keypoints.

**[TH]** ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ **MediaPipe Hands** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Machine Learning ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏∏‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î 3 ‡∏°‡∏¥‡∏ï‡∏¥ (3D Landmarks) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 21 ‡∏à‡∏∏‡∏î‡∏ö‡∏ô‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
- **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ù‡πà‡∏≤‡∏°‡∏∑‡∏≠:** ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏°‡∏∑‡∏≠‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
- **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏°‡∏∑‡∏≠:** ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏∑‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

![Hand Landmarks](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png)

### 2. Feature Extraction (‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞)
**[EN]** Raw coordinates (x, y) from the camera are not suitable for direct classification due to position and scale variations. The system performs preprocessing:
- **Translation Invariance:** All points are shifted relative to the Wrist (Point 0) so that the hand's position on the screen doesn't affect recognition.
- **Scale Invariance:** Coordinates are normalized by the hand's size to ensure consistent recognition regardless of distance from the camera.

**[TH]** ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏î‡∏¥‡∏ö (x, y) ‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÅ‡∏Å‡πà‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:
- **‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (Translation Invariance):** ‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏∑‡∏≠ (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 0) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏°‡∏∑‡∏≠‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å
- **‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î (Scale Invariance):** ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ù‡πà‡∏≤‡∏°‡∏∑‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏°‡∏∑‡∏≠‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏Å‡∏•‡∏Å‡∏•‡πâ‡∏≠‡∏á

### 3. K-Nearest Neighbors (KNN) Classification
**[EN]** For the "Teachable" feature, the system uses the **K-Nearest Neighbors (KNN)** algorithm. It is a non-parametric, lazy learning algorithm.
- When a user saves a gesture, the extracted feature vector is stored in memory.
- During inference, the system calculates the **Euclidean Distance** between the current hand pose and all stored samples.
- It selects the `K` closest samples (Neighbors) and assigns the class label based on a majority vote.

**[TH]** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå "‡∏™‡∏≠‡∏ô‡πÑ‡∏î‡πâ" ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° **K-Nearest Neighbors (KNN)** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö Lazy Learning
- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Feature Vector) ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
- ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏¢‡∏π‡∏Ñ‡∏•‡∏¥‡∏î (Euclidean Distance)** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡πà‡∏≤‡∏°‡∏∑‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô `K` ‡∏ï‡∏±‡∏ß (Neighbors) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å (Majority Vote)

---

## üõ† Installation & Usage (‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)

### Prerequisites (‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ)
- Python 3.9+
- Webcam

### Steps (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)
1. **Clone Repository:**
   ```bash
   git clone https://github.com/NKSR22/AI-Hand-Gesture-Recognition.git
   cd AI_Count_figer
   ```

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Application:**
   ```bash
   python main.py
   ```

### üéÆ How to Use (‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
- **Heuristic Mode (Default):** The system counts fingers using standard geometry logic (Yellow status).
- **Training Mode:**
  1. Pose your hand in front of the camera.
  2. Press a number key **(0-9)** to label that pose.
  3. The system switches to **AI Mode** (Green status) and starts recognizing your custom gestures.
- **Clear Data:** Press **'C'** to reset training data.
- **Exit:** Click the **EXIT button** or press **'Q'**.

---

## üê≥ Docker Guide (‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô Docker)
**[EN]** Running GUI applications with Webcam access in Docker requires **X11 Forwarding** configuration.
**[TH]** ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ GUI ‡πÅ‡∏•‡∏∞ Webcam ‡∏ú‡πà‡∏≤‡∏ô Docker ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ X11 Forwarding ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Container ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ

### Step 1: Build Image
‡∏™‡∏£‡πâ‡∏≤‡∏á Docker Image ‡∏à‡∏≤‡∏Å Dockerfile (‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å OS)
```bash
docker build -t ai-hand-tracker .
```

### Step 2: Prepare & Run Container (OS Specific Setup)

#### ü™ü Windows (WSL2)
**Requirement:** [VcXsrv Windows X Server](https://sourceforge.net/projects/vcxsrv/) & [usbipd-win](https://github.com/dorssel/usbipd-win)
1. **Setup XLaunch (VcXsrv):**
   - ‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° **XLaunch**
   - **Display settings:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `Multiple windows`
   - **Client startup:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `Start no client`
   - **Extra settings:** **‡∏ï‡∏¥‡πä‡∏Å‡∏ñ‡∏π‡∏Å** ‡∏ó‡∏µ‡πà `Disable access control` (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)

2. **Mount Webcam (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ WSL):**
   - ‡πÄ‡∏õ‡∏¥‡∏î PowerShell (Admin) ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô `usbipd list` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π BusID ‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á
   - ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: `usbipd wsl attach --busid <BUSID> --distribution Ubuntu` (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Ubuntu ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Distro ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)

3. **Run Command (‡πÉ‡∏ô WSL Terminal):**
   ```bash
   # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ IP ‡∏Ç‡∏≠‡∏á Display
   export DISPLAY=$(grep nameserver /etc/resolv.conf | awk '{print $2}'):0
   
   # ‡∏£‡∏±‡∏ô Container
   docker run -it --rm --device /dev/video0 -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix ai-hand-tracker
   ```

#### üçé macOS (Intel / Apple Silicon)
**Requirement:** [XQuartz](https://www.xquartz.org/)
1. **Setup XQuartz:**
   - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î **XQuartz**
   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà **Preferences** > **Security** > ‡∏ï‡∏¥‡πä‡∏Å‡∏ñ‡∏π‡∏Å `Allow connections from network clients`
   - **Restart** ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Mac ‡∏´‡∏£‡∏∑‡∏≠ Log out/Log in 1 ‡∏£‡∏≠‡∏ö

2. **Run Command:**
   ```bash
   # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ X11
   xhost + 127.0.0.1
   
   # ‡∏£‡∏±‡∏ô Container
   docker run -it --rm -e DISPLAY=host.docker.internal:0 ai-hand-tracker
   ```
   *(‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ö‡∏ô macOS, Docker Desktop ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Webcam (USB Passthrough) ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏´‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Local Python ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Network Camera)*

#### üêß Linux (Ubuntu/Debian)
‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Native Hardware
1. **Allow X11 Access:**
   ```bash
   xhost +local:docker
   ```
2. **Run Command:**
   ```bash
   docker run -it --rm --device /dev/video0 -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix ai-hand-tracker
   ```

### üê≥ Docker Compose (Recommended / ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
**[TH]** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏≤‡∏ß‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ `docker-compose` ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
‡πÑ‡∏ü‡∏•‡πå `docker-compose.yml` ‡πÑ‡∏î‡πâ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å OS ‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

1. **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå `docker-compose.yml` (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Mac/Linux)**
   - **Windows:** ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
   - **macOS:** ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏õ‡∏¥‡∏î Comment ‡∏Ç‡∏≠‡∏á Windows ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î Comment ‡∏Ç‡∏≠‡∏á macOS ‡πÅ‡∏ó‡∏ô

2. **‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**
   ```bash
   docker-compose up --build
   ```

---
**¬© 2024 Nakarin Sripanya.** All Rights Reserved.
